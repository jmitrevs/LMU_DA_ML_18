{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Challenge Example using Neural Networks\n",
    "In this part we will look at the **[Higgs Boson ML Challenge](https://www.kaggle.com/c/Higgs-boson)** on Kaggle and attempt a solution using neural networks (NN). The data is available from **[CERN Open Data](http://opendata.cern.ch/record/328)**. More information about the data is available from the links, and in particular at **[Documentation](http://opendata.cern.ch/record/329/files/atlas-higgs-challenge-2014.pdf)**. The general idea is that we want to extract $H\\to\\tau\\tau$ signal from background. In particular, the selection requires one of the taus to decay into an electron or muon and two neutrinos, and the other into hadrons and a neutrino. The challenge is based on Monte Carlo events processed through the **[ATLAS detector](http://atlas.cern/)** simulation and reconstruction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Neural Networks\n",
    "(based on lectures from **[ML Course on Coursera](https://www.coursera.org/learn/machine-learning)**)\n",
    "\n",
    "As we saw from the logistic regression yesterday, linear classifiers are often not the best at solving complicated problems. Neural networks introduce nonlinearity. They were originally designed to mimic the brain, and were popular in the 80s and early 90s. Recently they have become popular again, especially as deep neural networks DNNs, including convolutional NNs (CNN), recurrent NNs (RNN), etc. Those are beyond the scope of this class, but we will introduce the basics of NNs.\n",
    "\n",
    "Below is a diagram of a simple NN:\n",
    "![NNFig](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n",
    "It is made up of \"neurons\" that get a number of inputs, processes them, and sends the output to other neurons. Mathematically, one can represent the a neuron's \"activation\" $a = g\\left(\\theta^Tx\\right)$, where $x$ are the inputs (a vector), and $\\theta$ are the parameters (weights) of the model (also a vector), and $g$ is the activation fuction. For example, if we use a logistic function as the activation function, we can have $g\\left(\\theta^Tx\\right) = \\frac{1}{1+\\mathrm{exp}\\left(-\\theta^Tx\\right)}$, or if a Rectified Linear Unit (ReLU), $g\\left(\\theta^Tx\\right) = \\mathrm{max}\\left(0, \\theta^Tx\\right)$. The NN above has an input layer (layer 1), a hidden layer (layer 2), and an output layer (layer 3). One can have more hidden layers. Let's label the activations of layer 2 as $a_i^{(2)} = g\\left(\\theta_i^{(1)T}x\\right)$, where $i$ is the index of the individual neurons. Note that the superscript of the $\\theta$ is (1). That is because these are the weights going from layer 1 to 2. Putting together all the individual weight vectors together forms a matrix $\\Theta^{(1)}$.\n",
    "\n",
    "Using matrix notation, we can define $z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}$ and then $a^{(j)} = g(z^{(j)})$. Thus evaluating the NN is a series of matrix multiplications followed by activation functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function of a NN is similar to what we have for logistic regression, modified to take into account possible multiple outputs, and with more complicated regularization. In order to train the NN, we have to determine the weight matrix $\\Theta$ that minimizes the cost function. Backpropagation is the method used to do that. It calculates the partial derivatives \"errors\" for each $z_i^{(j)}$ by propagating the errors backwards. Usually something like (stochastic) gradient descent is used to solve the problem. For more details on backprorpagation, look, for example, at the **[ML course](https://www.coursera.org/learn/machine-learning)** mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start trying to apply a NN to the Higgs Challenge data. We will start using Scikit Learn, and then try **[Keras](https://keras.io/)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual setup: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv('data/atlas-higgs-challenge-2014-v2.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>b</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  ...  PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928  ...                2.150   \n",
       "1               3.473       2.078  ...                0.725   \n",
       "2               3.148       9.336  ...                2.053   \n",
       "3               3.310       0.414  ...             -999.000   \n",
       "4               3.891      16.405  ...             -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      s          t   \n",
       "1                -999.000          46.226  0.681042      b          t   \n",
       "2                -999.000          44.251  0.715742      b          t   \n",
       "3                -999.000          -0.000  1.660654      b          t   \n",
       "4                -999.000           0.000  1.904263      b          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more information about the variables in the documentation. The variables that start with **DER** are derived quantities, determined by the physicists performing the analysis as variables that discriminate signal from backround. On the other hand, those that start with **PRI** are considered to be primary variables, from which the derived variables are calculated. They themselves generally do not provide much discrimination, but one if the ideas suggested by deep networks is that they can determine the necessary features from the primary variables, potentially even finding variables that the physicists did not consider. *EventId* identifies the event but is not a \"feature.\" The *Weight* is the event weight so that the sum of weights of all signal events should produce the signal yield expected to be observed in 2012, and the sum of weights of all background events should produce the backgroudn yield. Note that the weight varies event to event, because different background and signal processes contribute to the background and signal sets. *Label* indicates if it is a signal or background event. Ignore the *Kaggle* variables--they are only used if you want to reproduce exactly what was used in the Challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map y values to integers\n",
    "df['Label'] = df['Label'].map({'b':0, 's':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0   100000       138.470                       51.655        97.827    27.980   \n",
       "1   100001       160.937                       68.768       103.235    48.146   \n",
       "2   100002      -999.000                      162.172       125.953    35.635   \n",
       "3   100003       143.905                       81.417        80.943     0.414   \n",
       "4   100004       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  ...  PRI_jet_leading_eta  \\\n",
       "0               3.064      41.928  ...                2.150   \n",
       "1               3.473       2.078  ...                0.725   \n",
       "2               3.148       9.336  ...                2.053   \n",
       "3               3.310       0.414  ...             -999.000   \n",
       "4               3.891      16.405  ...             -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt    Weight  Label  KaggleSet  \\\n",
       "0                  -2.475         113.497  0.000814      1          t   \n",
       "1                -999.000          46.226  0.681042      0          t   \n",
       "2                -999.000          44.251  0.715742      0          t   \n",
       "3                -999.000          -0.000  1.660654      0          t   \n",
       "4                -999.000           0.000  1.904263      0          t   \n",
       "\n",
       "   KaggleWeight  \n",
       "0      0.002653  \n",
       "1      2.233584  \n",
       "2      2.347389  \n",
       "3      5.446378  \n",
       "4      6.245333  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create separate arrays\n",
    "eventID = df['EventId']\n",
    "X = df.loc[:,'DER_mass_MMC':'PRI_jet_all_pt']\n",
    "y = df['Label']\n",
    "weight = df['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now split into testing and training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, eventID_train, event_ID_test, weight_train, weight_test = train_test_split(\n",
    "    X, y, eventID, weight, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's first look at a NN in sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(early_stopping=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and train\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8183720404860398"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kaggle competition used the approximate median segnificance (AMS), as defined below, to determine how good a solution was. The number 10, added to the background yield, is a regularization term to decrease the variance of the AMS.\n",
    "\n",
    "Note that if you do not use the full data set (i.e. you split into training and testing) you have to reweigh the inputs so that the subsample yield matches to the toal yield, which we will do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute approximate median segnificance (AMS)\n",
    "\n",
    "def ams(s,b):\n",
    "    from math import sqrt,log\n",
    "    if b==0:\n",
    "        return 0\n",
    "\n",
    "    return sqrt(2*((s+b+10)*log(1+float(s)/(b+10))-s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob = mlp.predict_proba(X_train)[:, 1]\n",
    "y_test_prob = mlp.predict_proba(X_test)[:, 1]\n",
    "pcut = np.percentile(y_train_prob,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the probability to the original data frame\n",
    "df['Prob']=mlp.predict_proba(X)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10de65950>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n",
    "\n",
    "df[df.Label==0].Prob.hist(label='Background',**kwargs)\n",
    "df[df.Label==1].Prob.hist(label='Signal',**kwargs)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the total weights (yields)\n",
    "sigall = weight.dot(y)\n",
    "backall = weight.dot(y == 0)\n",
    "\n",
    "# The training weights\n",
    "sigtrain = weight_train.dot(y_train)\n",
    "backtrain = weight_train.dot(y_train == 0)\n",
    "\n",
    "# The training weights\n",
    "sigtest = weight_test.dot(y_test)\n",
    "backtest = weight_test.dot(y_test == 0)\n",
    "\n",
    "# aside:  these can also be done by looping instead of using a dot product\n",
    "#  (Usually vectorized operations are faster for interpreted code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel = weight_train.dot(y_train & (y_train_prob > pcut))\n",
    "backtrain_sel = weight_train.dot((y_train == 0) & (y_train_prob > pcut))\n",
    "\n",
    "sigtest_sel = weight_test.dot(y_test & (y_test_prob > pcut))\n",
    "backtest_sel = weight_test.dot((y_test == 0) & (y_test_prob > pcut))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 206.23344127972067, background = 5389.368110747649\n",
      "Corrected selected yields in test sample, signal = 204.52874804106585, background = 5497.837700307203\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_corr = sigtrain_sel*sigall/sigtrain\n",
    "backtrain_sel_corr = backtrain_sel*backall/backtrain\n",
    "\n",
    "sigtest_sel_corr = sigtest_sel*sigall/sigtest\n",
    "backtest_sel_corr = backtest_sel*backall/backtest\n",
    "\n",
    "print(f\"Corrected selected yields in training sample, signal = {sigtrain_sel_corr}, background = {backtrain_sel_corr}\")\n",
    "print(f\"Corrected selected yields in test sample, signal = {sigtest_sel_corr}, background = {backtest_sel_corr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 2.789057327350149\n",
      "AMS of test sample 2.7391044153222395\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_corr,backtrain_sel_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_corr,backtest_sel_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? Worse than the BDT from yesterday.\n",
    "![Comparison with submissions](data/tr150908_davidRousseau_TMVAFuture_HiggsML.001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are quite sensitive to feature scaling, so let's try to scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and train a new network\n",
    "mlp_scaled = MLPClassifier(early_stopping=True)\n",
    "mlp_scaled.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8376669789903674"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_scaled.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob_scaled = mlp_scaled.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_prob_scaled = mlp_scaled.predict_proba(X_test_scaled)[:, 1]\n",
    "pcut_scaled = np.percentile(y_train_prob_scaled,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel_scaled = weight_train.dot(y_train & (y_train_prob_scaled > pcut_scaled))\n",
    "backtrain_sel_scaled = weight_train.dot((y_train == 0) & (y_train_prob_scaled > pcut_scaled))\n",
    "\n",
    "sigtest_sel_scaled = weight_test.dot(y_test & (y_test_prob_scaled > pcut_scaled))\n",
    "backtest_sel_scaled = weight_test.dot((y_test == 0) & (y_test_prob_scaled > pcut_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 234.11649808661173, background = 4319.616803813221\n",
      "Corrected selected yields in test sample, signal = 233.46430828783664, background = 4652.135275642529\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_scaled_corr = sigtrain_sel_scaled*sigall/sigtrain\n",
    "backtrain_sel_scaled_corr = backtrain_sel_scaled*backall/backtrain\n",
    "\n",
    "sigtest_sel_scaled_corr = sigtest_sel_scaled*sigall/sigtest\n",
    "backtest_sel_scaled_corr = backtest_sel_scaled*backall/backtest\n",
    "\n",
    "print(f\"Corrected selected yields in training sample, signal = {sigtrain_sel_scaled_corr}, background = {backtrain_sel_scaled_corr}\")\n",
    "print(f\"Corrected selected yields in test sample, signal = {sigtest_sel_scaled_corr}, background = {backtest_sel_scaled_corr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 3.526647034997254\n",
      "AMS of test sample 3.3912701699219534\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_scaled_corr,backtrain_sel_scaled_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_scaled_corr,backtest_sel_scaled_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved somewhat.\n",
    "\n",
    "SciKit Learn has simple NNs, but if you want to do deep NNs, or train on GPUs, you probalby want to use something like TensorFlow instead. Let's try to create a simple NN using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from tensorflow.keras import models, layers, regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.0001), input_shape=(30,)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 548219 samples\n",
      "Epoch 1/20\n",
      "548219/548219 [==============================] - 9s 17us/sample - loss: 0.4073 - accuracy: 0.8186\n",
      "Epoch 2/20\n",
      "548219/548219 [==============================] - 12s 22us/sample - loss: 0.3834 - accuracy: 0.8315\n",
      "Epoch 3/20\n",
      "548219/548219 [==============================] - 9s 17us/sample - loss: 0.3784 - accuracy: 0.8333\n",
      "Epoch 4/20\n",
      "548219/548219 [==============================] - 9s 16us/sample - loss: 0.3754 - accuracy: 0.8344\n",
      "Epoch 5/20\n",
      "548219/548219 [==============================] - 10s 18us/sample - loss: 0.3735 - accuracy: 0.8352\n",
      "Epoch 6/20\n",
      "548219/548219 [==============================] - 8s 15us/sample - loss: 0.3723 - accuracy: 0.8353\n",
      "Epoch 7/20\n",
      "548219/548219 [==============================] - 7s 13us/sample - loss: 0.3717 - accuracy: 0.8356s - loss: 0.3716 - accuracy: 0.\n",
      "Epoch 8/20\n",
      "548219/548219 [==============================] - 7s 13us/sample - loss: 0.3712 - accuracy: 0.8360\n",
      "Epoch 9/20\n",
      "548219/548219 [==============================] - 8s 15us/sample - loss: 0.3704 - accuracy: 0.8362\n",
      "Epoch 10/20\n",
      "548219/548219 [==============================] - 10s 18us/sample - loss: 0.3702 - accuracy: 0.8365\n",
      "Epoch 11/20\n",
      "548219/548219 [==============================] - 8s 14us/sample - loss: 0.3700 - accuracy: 0.8361\n",
      "Epoch 12/20\n",
      "548219/548219 [==============================] - 7s 13us/sample - loss: 0.3695 - accuracy: 0.8365\n",
      "Epoch 13/20\n",
      "548219/548219 [==============================] - 13s 24us/sample - loss: 0.3694 - accuracy: 0.8364\n",
      "Epoch 14/20\n",
      "548219/548219 [==============================] - 8s 15us/sample - loss: 0.3693 - accuracy: 0.8365\n",
      "Epoch 15/20\n",
      "548219/548219 [==============================] - 9s 16us/sample - loss: 0.3689 - accuracy: 0.8369\n",
      "Epoch 16/20\n",
      "548219/548219 [==============================] - 10s 17us/sample - loss: 0.3688 - accuracy: 0.8369\n",
      "Epoch 17/20\n",
      "548219/548219 [==============================] - 10s 19us/sample - loss: 0.3688 - accuracy: 0.8368\n",
      "Epoch 18/20\n",
      "548219/548219 [==============================] - 11s 19us/sample - loss: 0.3687 - accuracy: 0.8365\n",
      "Epoch 19/20\n",
      "548219/548219 [==============================] - 10s 19us/sample - loss: 0.3684 - accuracy: 0.8366\n",
      "Epoch 20/20\n",
      "548219/548219 [==============================] - 10s 19us/sample - loss: 0.3683 - accuracy: 0.8371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10d97e390>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=128)\n",
    "# Note: this takes advantage of multiple cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a different probability cut, not the one given by default to predict().\n",
    "# We choose the top 15%, but can optimize\n",
    "y_train_prob_keras = model.predict(X_train_scaled)[:, 0]\n",
    "y_test_prob_keras = model.predict(X_test_scaled)[:, 0]\n",
    "pcut_keras = np.percentile(y_train_prob_keras,85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Label</th>\n",
       "      <th>KaggleSet</th>\n",
       "      <th>KaggleWeight</th>\n",
       "      <th>Prob</th>\n",
       "      <th>Prob_keras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.910</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>...</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.240</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.599515</td>\n",
       "      <td>0.421757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.681042</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.233584</td>\n",
       "      <td>0.327842</td>\n",
       "      <td>0.295592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "      <td>0.715742</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>2.347389</td>\n",
       "      <td>0.680071</td>\n",
       "      <td>0.069008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.660654</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>5.446378</td>\n",
       "      <td>0.042716</td>\n",
       "      <td>0.087216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.904263</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>6.245333</td>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.051772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818233</th>\n",
       "      <td>918233</td>\n",
       "      <td>105.668</td>\n",
       "      <td>46.443</td>\n",
       "      <td>60.048</td>\n",
       "      <td>156.191</td>\n",
       "      <td>0.403</td>\n",
       "      <td>47.746</td>\n",
       "      <td>0.936</td>\n",
       "      <td>1.279</td>\n",
       "      <td>6.133</td>\n",
       "      <td>...</td>\n",
       "      <td>41.791</td>\n",
       "      <td>0.787</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>154.056</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>1</td>\n",
       "      <td>u</td>\n",
       "      <td>0.259892</td>\n",
       "      <td>0.636080</td>\n",
       "      <td>0.395933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818234</th>\n",
       "      <td>918234</td>\n",
       "      <td>99.294</td>\n",
       "      <td>30.097</td>\n",
       "      <td>62.713</td>\n",
       "      <td>65.861</td>\n",
       "      <td>3.312</td>\n",
       "      <td>471.319</td>\n",
       "      <td>-2.611</td>\n",
       "      <td>2.294</td>\n",
       "      <td>2.889</td>\n",
       "      <td>...</td>\n",
       "      <td>70.158</td>\n",
       "      <td>-2.018</td>\n",
       "      <td>2.893</td>\n",
       "      <td>178.856</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>1</td>\n",
       "      <td>u</td>\n",
       "      <td>0.020956</td>\n",
       "      <td>0.545047</td>\n",
       "      <td>0.449726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818235</th>\n",
       "      <td>918235</td>\n",
       "      <td>108.497</td>\n",
       "      <td>9.837</td>\n",
       "      <td>65.149</td>\n",
       "      <td>18.006</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.742</td>\n",
       "      <td>18.006</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.189365</td>\n",
       "      <td>0</td>\n",
       "      <td>u</td>\n",
       "      <td>53.284258</td>\n",
       "      <td>0.527639</td>\n",
       "      <td>0.591838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818236</th>\n",
       "      <td>918236</td>\n",
       "      <td>96.711</td>\n",
       "      <td>20.006</td>\n",
       "      <td>66.942</td>\n",
       "      <td>29.761</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.479</td>\n",
       "      <td>2.739</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>30.863</td>\n",
       "      <td>0.512740</td>\n",
       "      <td>0</td>\n",
       "      <td>u</td>\n",
       "      <td>22.971060</td>\n",
       "      <td>0.390194</td>\n",
       "      <td>0.267853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818237</th>\n",
       "      <td>918237</td>\n",
       "      <td>92.373</td>\n",
       "      <td>80.109</td>\n",
       "      <td>77.619</td>\n",
       "      <td>3.984</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>2.486</td>\n",
       "      <td>3.984</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>1.531213</td>\n",
       "      <td>0</td>\n",
       "      <td>u</td>\n",
       "      <td>68.599269</td>\n",
       "      <td>0.019252</td>\n",
       "      <td>0.034684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>818238 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        EventId  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  \\\n",
       "0        100000       138.470                       51.655        97.827   \n",
       "1        100001       160.937                       68.768       103.235   \n",
       "2        100002      -999.000                      162.172       125.953   \n",
       "3        100003       143.905                       81.417        80.943   \n",
       "4        100004       175.864                       16.915       134.805   \n",
       "...         ...           ...                          ...           ...   \n",
       "818233   918233       105.668                       46.443        60.048   \n",
       "818234   918234        99.294                       30.097        62.713   \n",
       "818235   918235       108.497                        9.837        65.149   \n",
       "818236   918236        96.711                       20.006        66.942   \n",
       "818237   918237        92.373                       80.109        77.619   \n",
       "\n",
       "        DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0         27.980                 0.910           124.711                2.666   \n",
       "1         48.146              -999.000          -999.000             -999.000   \n",
       "2         35.635              -999.000          -999.000             -999.000   \n",
       "3          0.414              -999.000          -999.000             -999.000   \n",
       "4         16.405              -999.000          -999.000             -999.000   \n",
       "...          ...                   ...               ...                  ...   \n",
       "818233   156.191                 0.403            47.746                0.936   \n",
       "818234    65.861                 3.312           471.319               -2.611   \n",
       "818235    18.006              -999.000          -999.000             -999.000   \n",
       "818236    29.761              -999.000          -999.000             -999.000   \n",
       "818237     3.984              -999.000          -999.000             -999.000   \n",
       "\n",
       "        DER_deltar_tau_lep  DER_pt_tot  ...  PRI_jet_subleading_pt  \\\n",
       "0                    3.064      41.928  ...                 46.062   \n",
       "1                    3.473       2.078  ...               -999.000   \n",
       "2                    3.148       9.336  ...               -999.000   \n",
       "3                    3.310       0.414  ...               -999.000   \n",
       "4                    3.891      16.405  ...               -999.000   \n",
       "...                    ...         ...  ...                    ...   \n",
       "818233               1.279       6.133  ...                 41.791   \n",
       "818234               2.294       2.889  ...                 70.158   \n",
       "818235               2.742      18.006  ...               -999.000   \n",
       "818236               2.479       2.739  ...               -999.000   \n",
       "818237               2.486       3.984  ...               -999.000   \n",
       "\n",
       "        PRI_jet_subleading_eta  PRI_jet_subleading_phi  PRI_jet_all_pt  \\\n",
       "0                        1.240                  -2.475         113.497   \n",
       "1                     -999.000                -999.000          46.226   \n",
       "2                     -999.000                -999.000          44.251   \n",
       "3                     -999.000                -999.000          -0.000   \n",
       "4                     -999.000                -999.000           0.000   \n",
       "...                        ...                     ...             ...   \n",
       "818233                   0.787                  -1.090         154.056   \n",
       "818234                  -2.018                   2.893         178.856   \n",
       "818235                -999.000                -999.000          -0.000   \n",
       "818236                -999.000                -999.000          30.863   \n",
       "818237                -999.000                -999.000          -0.000   \n",
       "\n",
       "          Weight  Label  KaggleSet  KaggleWeight      Prob  Prob_keras  \n",
       "0       0.000814      1          t      0.002653  0.599515    0.421757  \n",
       "1       0.681042      0          t      2.233584  0.327842    0.295592  \n",
       "2       0.715742      0          t      2.347389  0.680071    0.069008  \n",
       "3       1.660654      0          t      5.446378  0.042716    0.087216  \n",
       "4       1.904263      0          t      6.245333  0.010931    0.051772  \n",
       "...          ...    ...        ...           ...       ...         ...  \n",
       "818233  0.005721      1          u      0.259892  0.636080    0.395933  \n",
       "818234  0.000461      1          u      0.020956  0.545047    0.449726  \n",
       "818235  1.189365      0          u     53.284258  0.527639    0.591838  \n",
       "818236  0.512740      0          u     22.971060  0.390194    0.267853  \n",
       "818237  1.531213      0          u     68.599269  0.019252    0.034684  \n",
       "\n",
       "[818238 rows x 37 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the probability to the original data frame\n",
    "df['Prob_keras']=model.predict(scaler.transform(X))[:, 0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10d78b6d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n",
    "\n",
    "df[df.Label==0].Prob_keras.hist(label='Background',**kwargs)\n",
    "df[df.Label==1].Prob_keras.hist(label='Signal',**kwargs)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at event yields that pass our selection\n",
    "sigtrain_sel_keras = weight_train.dot(y_train & (y_train_prob_keras > pcut_keras))\n",
    "backtrain_sel_keras = weight_train.dot((y_train == 0) & (y_train_prob_keras > pcut_keras))\n",
    "\n",
    "sigtest_sel_keras = weight_test.dot(y_test & (y_test_prob_keras > pcut_keras))\n",
    "backtest_sel_keras = weight_test.dot((y_test == 0) & (y_test_prob_keras > pcut_keras))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected selected yields in training sample, signal = 246.9416715620902, background = 5111.378984261147\n",
      "Corrected selected yields in test sample, signal = 244.97787214583164, background = 5400.100814959756\n"
     ]
    }
   ],
   "source": [
    "# Now we need to correct the selected yields to be is if we used the full sample\n",
    "sigtrain_sel_keras_corr = sigtrain_sel_keras*sigall/sigtrain\n",
    "backtrain_sel_keras_corr = backtrain_sel_keras*backall/backtrain\n",
    "\n",
    "sigtest_sel_keras_corr = sigtest_sel_keras*sigall/sigtest\n",
    "backtest_sel_keras_corr = backtest_sel_keras*backall/backtest\n",
    "\n",
    "print(f\"Corrected selected yields in training sample, signal = {sigtrain_sel_keras_corr}, background = {backtrain_sel_keras_corr}\")\n",
    "print(f\"Corrected selected yields in test sample, signal = {sigtest_sel_keras_corr}, background = {backtest_sel_keras_corr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMS of training sample 3.4234623596282696\n",
      "AMS of test sample 3.3059398454595024\n"
     ]
    }
   ],
   "source": [
    "print(\"AMS of training sample\", ams(sigtrain_sel_keras_corr,backtrain_sel_keras_corr))\n",
    "print(\"AMS of test sample\", ams(sigtest_sel_keras_corr,backtest_sel_keras_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only made a single layer NN in TensorFlow. However, you can easily change the structure of the network. As an assignment, try adding an extra hidden layer and changing the number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: TensorFlow has added an example playing with data from the same dataset: https://www.tensorflow.org/tutorials/keras/overfit_and_underfit. It is newer than the one in this notebook, and shows nice ways to check for overfitting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
